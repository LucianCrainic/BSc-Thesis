% ============================================== %
%
% Results and Discussion Chapter
%
%% ============================================== %
\hypersetup{colorlinks=true, linkcolor=red}
\hypersetup{citecolor=blue}

\chapter{Results and Discussion}

    This chapter presents the result obtained from the expirements conducted in the previous chapter. Classification models are evaluated using different approaches and metrics. The results are then discussed and compared to each other.

    \section{Models evaluation}

        Performed using the \textit{Scikit-Learn} library \cite{sklearn_api}. It provides a wide range of validation methods and metrics to evaluate the performance of the models. The following sections will present the validation methods and metrics used in this thesis.

        \subsection{Validation}

            Validation is the process of evaluating the perfomance of the models. The goal of validation is to estimate the performance of the model on new data, not used during the training process. The following validation methods are used:

            \subsubsection{Hold-Out}

                This method is widely used for for its simplicity and speed. The dataset is split into two subsets. The Training set is used to train the model, Testing set is used to evaluate the performance of the model. Typically, a common split ratio is:
                \begin{itemize}
                    \item \textbf{Training set}: 70\% of the dataset.
                    \item \textbf{Testing set}: 30\% of the dataset.
                \end{itemize}
            
            \subsubsection{Cross-Validation}
                
                This method is used in the literature for its effectiveness and robustness. It can be time consuming for large datasets, but it is the best method to evaluate the performance of the models.

                \begin{itemize}

                    \item \textbf{K-fold}: The data is divided into \textit{K} folds, then \textit{K-1} folds are used for training and the remaining fold is used for testing. This process is repeated \textit{K} times, with each fold being used exactly once for testing. The fig:kfold shows an example of a k-fold split. Fig \ref{fig:kfold} shows the KFold split.
                    
                    \begin{figure}[H]
                        \centering
                        \includegraphics[width=\textwidth,height=5cm,keepaspectratio=true]{../src/resources/kfold.png}
                        \caption{
                          KFold Visualization from the scikit-learn documentation \cite{scikit-learn}.
                        }
                        \label{fig:kfold}
                    \end{figure}

                    \item \textbf{Group-k-fold}: Variation of k-fold designed for situations where the data has inherent groupings or dependecies that should be preserved in the train/test split. In this method, the data is divided into \textit{K} folds, then an additional constraint is imposed to ensure that data point from the same group are in the same fold. Fig \ref{fig:groupkfold} shows the GroupKFold split.
                    
                    \begin{figure}[H]
                        \centering
                        \includegraphics[width=\textwidth,height=5cm,keepaspectratio=true]{../src/resources/groupkfold.png}
                        \caption{
                          GroupKFold Visualization from the scikit-learn documentation \cite{scikit-learn}.
                        }
                        \label{fig:groupkfold}
                    \end{figure}
                \end{itemize}
                
        The advantage of using \textit{Cross-Validation} over \textit{Hold-Out} is that all the samples are used for both training and testing, and each sample is used for testing exactly once. This method helps to reduce the variance of the estimated performance of the model, by averaging the results over a number of trials. The disadvantage of using Cross-Validation is that it is computationally expensive for very large datasets. \\ 

        In this study, both methods are used to evaluate the performance of the models. The Hold-Out method is used to evaluate the perfomance of the models for the \textit{Wrong approach} and \textit{Sequence approach} datasets due to their large dimension. The Cross-Validation method is used  with the Hold-Out method to evaluate the perfomance of the models for the \textit{Correct approach} and \textit{Feature Engineering approach} dataset due to them scoring the best results and being the effective approaches. Confronting the results of the two methods will show the correctness of the evaluation, as the results should be similar. \\
        
        \subsection{Metrics}

            This section will report the metrics used to benchmark the different models used in this study.

            \subsubsection{Accuracy score}

                The \textit{accuracy} is the proportion of correct predictions, considering both true positives and true negatives, among the total number of samples. The formula used to calculate the accuracy is the following:
                \begin{equation}
                    \frac{TP + TN}{TP + TN + FP + FN}
                \end{equation} 
                where \textbf{TP} is the number of true positives, \textbf{TN} is the number of true negatives, \textbf{FP} is the number of false positives and \textbf{FN} is the number of false negatives.

            \subsubsection{Precision score}

                The \textit{precision} is the ability of the classifier not to label as positive a sample that is negative. The formula used to calculate the precision is the following:
                \begin{equation}
                    \frac{TP}{TP + FP}
                \end{equation}

            \subsubsection{Recall score}

                The \textit{recall} is the ability of the classifier to find all the positive samples. The formula used to calculate the recall is the following:
                \begin{equation}
                    \frac{TP}{TP + FN}
                \end{equation}

            \subsubsection{F1 score}

                The \textit{F1 score} is the harmonic mean of the precision and recall. The formula used to calculate the F1 score is the following:
                \begin{equation}
                    \frac{ 2 \times (precision \times recall)}{precision + recall}
                \end{equation}

            \subsubsection{Matthews correlation coefficent} 

                The \textit{Matthews correlation coefficent} (or $\varphi$ coefficient) takes into account true and false positives and negatives and is regarded as a balanced measure which can be used even if the classes are of very different sizes. The formula used to calculate the $\varphi$ coefficient is as follows: 
                \begin{equation}
                    \frac{TP \times TN - FP \times FN}{\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}
                \end{equation}

            These metrics will be used to show the effectiveness of the approaches proposed in this thesis.
    
\section{Evaluation results}
        
        Combining the validation methods and metrics presented above, the following tables will show the results obtained from the expirements conducted. The results are divided into two categories: \textbf{Exploratory} shows two approaches that were tested but did not obtain good results due to wrong implementation or loss of information. \textbf{Effective} shows two approaches that obtained good results and are suitable for this task. The results are presented in the following order: \textit{Traditional Approach}, \textit{Sequence Approach}, \textit{Effective Approach} and \textit{Feature Engineering Approach}. 

        \subsection{Exploratory}

            The following approaches are included because they a starting point in this thesis, and show the how different approaches can affect the accuracy of the models.
            
            \subsubsection{Traditional Approach}
                
                This approach was the first one to be tested and it got surprisingly good results. Such a simple approach and yet high accuracy raised doubts about the validity of the results, after further investigation it was discovered that the dataset was not properly split into training and testing sets as stated in Chapter \ref{sec:badsplit}. 
            
                \begin{table}[htbp]
                    \centering
                    \caption{Evaluation results using \textbf{Hold-Out} validation method.}
                    \label{tab:wrong_approach_holdout}
                    \begin{tabular}{lrrrrr}
                        \toprule
                        \textbf{Model} & \textbf{Accuracy} & \textbf{F1} & \textbf{Recall} & \textbf{Precision} & \textbf{MCC} \\
                        \midrule
                        Random Forest & \textbf{0.9958} & \textbf{0.9948} & \textbf{0.9950} & \textbf{0.9947} & \textbf{0.9953} \\
                        K-Nearest Neighbor & 0.9847 & 0.9811 & 0.9814 & 0.9809 & 0.9826 \\
                        Decision Tree & 0.9651 & 0.9596 & 0.9599 & 0.9594 & 0.9605 \\
                        Support-Vector Machines & 0.8770 & 0.8605 & 0.8586 & 0.8667 & 0.8610 \\
                        Logistic Regression & 0.8264 & 0.8048 & 0.8022 & 0.8097 & 0.8035 \\
                        \bottomrule
                    \end{tabular}
                \end{table}

                In Table \ref{tab:wrong_approach_holdout} the results obtained from the Hold-Out method are shown. High values are obtained for all the metrics, with \textbf{Random Forest} obtaining the highest values with a score of \textbf{0.9958} for accuracy. This confirmed the doubts about the validity of the results, a patient is both present in the training and testing set. This led to the models overfitting the data and obtaining high accuracy.

            \subsubsection{Sequence Approach}

                This approach got the lowest results of all the approaches. It was tested to see if concatenating the frames of a movement into a sequence would help the models differentiate between movements as stated in Chapter \ref{sec:seqsplit}.
                
                \begin{table}[htbp]
                    \centering
                    \caption{Evaluation results using \textbf{Hold-Out} validation method.}
                    \label{tab:sequence_approach_holdout}
                    \begin{tabular}{lrrrrr}
                        \toprule
                        \textbf{Model} & \textbf{Accuracy} & \textbf{F1} & \textbf{Recall} & \textbf{Precision} & \textbf{MCC} \\
                        \midrule
                        K-Nearest Neighbor & \textbf{0.5659} & \textbf{0.5414} & \textbf{0.5474} & \textbf{0.5497} & \textbf{0.5158} \\
                        Random Forest & 0.5463 & 0.5289 & 0.5272 & 0.5550 & 0.4932 \\
                        Support-Vector Machines & 0.5268 & 0.4716 & 0.4924 & 0.4677 & 0.4727 \\
                        LogisticRegression & 0.4439 & 0.4156 & 0.4274 & 0.4352 & 0.3812 \\
                        Decision Tree & 0.4390 & 0.4137 & 0.4190 & 0.4262 & 0.3749 \\
                        \bottomrule
                    \end{tabular}
                \end{table}

                In Table \ref{tab:sequence_approach_holdout} the results obtained from the Hold-Out method are shown. Low values are obtained for all the metrics, with \textbf{K-Nearest Neighbor} obtaining the highest values with a score of \textbf{0.5659} for accuracy. This results are considered low based on other approaches, however in the context of randomly guessing the movement of a patient the accuracy would be \textbf{0.1} as there are 10 movements. This means that the models are able to differentiate between movements, but not with a high accuracy. This approach was not used in the other approaches as it was not able to obtain a high accuracy.

        \subsection{Effective}
                The following approaches are the ones that obtained the best results and are suitable for this task. The main difference between the two approaches is the data used to train the models. The \textit{Correct Approach} uses the data as it is from the Kinect sensor, while the \textit{Feature Engineering Approach} uses the data after applying feature engineering techniques.
                
            \subsubsection{Effective Approach}
                This approach is considered effective because the raw kinect data is able to obtain a high accuracy. The data is not modified in any way, beside the removal of rotational data and pre-processing the data to remove noise as described in Chapter \ref{sec:goodsplit}.\\
                 
                In Table \ref{tab:correct_approach_holdout} the results obtain from the Hold-Out method are shown. \textbf{Random Forest} obtains the highest values for all the metrics, with a score of \textbf{0.7461} for accuracy. Other models such as \textit{Gradient Boosting}, \textit{Linear Discriminant Analysis}, \textit{Support-Vector Machines}, \textit{K-Nearest Neighbors} obtained great results as well with a score greater than \textbf{0.70} for accuracy. This confirms that the data obtained from the Kinect sensor is suitable for the task of movement classification without any major tweaks.\\
                
                \newpage
                \begin{table}[htbp]
                    \centering
                    \caption{Evaluation results using \textbf{Hold-Out} validation method.}
                    \label{tab:correct_approach_holdout}
                    \begin{tabular}{lrrrrr}
                        \toprule
                        \textbf{Model} & \textbf{Accuracy} & \textbf{F1} & \textbf{Recall} & \textbf{Precision} & \textbf{MCC} \\
                        \midrule
                        Random Forest & \textbf{0.7461} & \textbf{0.7322} & \textbf{0.7386} & \textbf{0.7302} & \textbf{0.7124} \\
                        Gradient Boosting & 0.7344 & 0.7230 & 0.7260 & 0.7248 & 0.6989 \\
                        Linear Discriminant Analysis & 0.7232 & 0.7157 & 0.7111 & 0.7421 & 0.6887 \\
                        Support-Vector Machines & 0.7187 & 0.7182 & 0.7155 & 0.7274 & 0.6807 \\
                        KNN & 0.7101 & 0.6971 & 0.6980 & 0.7031 & 0.6716 \\
                        Logistic Regression & 0.6688 & 0.6467 & 0.6488 & 0.6468 & 0.6244 \\
                        Decision Tree & 0.6553 & 0.6328 & 0.6468 & 0.6317 & 0.6109 \\
                        Naive Bayes & 0.6333 & 0.6077 & 0.6105 & 0.6226 & 0.5840 \\
                        Multi-Layer Perceptron & 0.6278 & 0.5962 & 0.6241 & 0.6131 & 0.5840 \\
                        AdaBoost & 0.3563 & 0.2209 & 0.2888 & 0.2405 & 0.3205 \\
                        \bottomrule
                    \end{tabular}
                \end{table}

                In Table \ref{tab:correct_approach_holdout} \textbf{Hold-Out} validation method was used for all the models, while in Table \ref{tab:correct_approach_cv} \textbf{Cross-Validation} was used with only 3 models to compare the two validation methods and show that there is no major difference between them. The results obtained from the two methods are similar, with the Cross-Validation method obtaining slightly lower values for all the metrics. This is due to the fact that the Cross-Validation method is more robust and the results are averaged over a number of trials. \\

                Another observation is that the Hold-Out method was used for it's speed, with \textbf{10 minutes} of training time while cross-validation was run for hours without finishing. This is due to the fact that this approaches uses the raw Kinect data, which contains over \textbf{59000} rows and \textbf{100} columns. 

                % In Table \ref{tab:correct_approach_holdout} \textbf{Hold-Out} validation method was used for all the models, while in Table \ref{tab:correct_approach_cv} \textbf{Cross-Validation} was used with only 3 models to compare the two validation methods and show that there is no major difference between them. The results obtained from the two methods are similar, with the Cross-Validation method obtaining slightly lower values for all the metrics. This is due to the fact that the Cross-Validation method is more robust and the results are averaged over a number of trials. \\
                %! Cross-Validation vs Hold-Out in Correct Approach
                \begin{table}[ht]
                    \centering
                    \caption{CV = Cross-Validation, HO = Hold-Out}
                    \label{tab:correct_approach_cv}
                    \begin{tabular}{lrrrrr}
                        \toprule
                        \textbf{Model} & \textbf{Accuracy} & \textbf{F1} & \textbf{Recall} & \textbf{Precision} & \textbf{MCC} \\
                        \midrule
                        LDA (CV) & 0.6362 & 0.6169 & 0.6025 & 0.6732 & 0.5942 \\
                        LDA (HO) & 0.6362 & 0.6156 & 0.5987 & 0.6541 & 0.5905 \\
                        KNN (CV) & 0.6127 & 0.5832 & 0.5919 & 0.5917 & 0.5632 \\
                        KNN (HO) & 0.6292 & 0.5941 & 0.5971 & 0.5974 & 0.5788 \\
                        Naive Bayes (CV) & 0.5884 & 0.5551 & 0.5621 & 0.6176 & 0.5414 \\
                        Naive Bayes (HO) & 0.5747 & 0.5576 & 0.5538 & 0.5980 & 0.5180 \\
                        \bottomrule
                        \end{tabular}
                \end{table}
                
                \newpage 
                \begin{table}[htbp]
                    \centering
                    \begin{minipage}{\linewidth}
                        \centering
                        \begin{tabular}{lrrrrr}
                            \toprule
                            \textbf{Model} & \textbf{Accuracy} & \textbf{F1 Score} & \textbf{Recall} & \textbf{Precision} & \textbf{MCC} \\
                            \midrule
                            Gradient Boosting & \textbf{0.7137} & \textbf{0.7059} & \textbf{0.7133} & \textbf{0.7048} & \textbf{0.6711} \\
                            SVM & 0.7024 & 0.6860 & 0.6976 & 0.6909 & 0.6594 \\
                            LDA & 0.6933 & 0.6829 & 0.6725 & 0.7233 & 0.6513 \\
                            \bottomrule
                        \end{tabular}
                        \caption{Table 1}
                        \vspace{5pt} 
                
                        \begin{tabular}{lrrrrr}
                            \toprule
                            \textbf{Model} & \textbf{Accuracy} & \textbf{F1 Score} & \textbf{Recall} & \textbf{Precision} & \textbf{MCC} \\
                            \midrule
                            SVM & \textbf{0.7073} & \textbf{0.6908} & \textbf{0.6984} & \textbf{0.6942} & \textbf{0.6630} \\
                            LDA & 0.6890 & 0.6732 & 0.6651 & 0.7039 & 0.6452 \\
                            Gradient Boosting & 0.6887 & 0.6798 & 0.6866 & 0.6817 & 0.6421 \\
                            \bottomrule
                        \end{tabular}
                        \caption{Table 2}
                    \end{minipage}
                \end{table}

            \subsubsection{Feature Engineering Approach}
            %! Feature Engineering Approach
            % \begin{table}[ht]
            %     \centering
            %     \begin{tabular}{lrrrrr}
            %         \toprule
            %         \textbf{Model} & \textbf{Accuracy} & \textbf{F1 Score} & \textbf{Recall} & \textbf{Precision} & \textbf{MCC} \\
            %         \midrule
            %         MLP & \textbf{0.8291}& \textbf{0.8375} & \textbf{0.8426} & \textbf{0.8472} & \textbf{0.8111} \\
            %         LDA & 0.8217 & 0.8125 & 0.8183 & 0.8298 & 0.8040 \\
            %         Logistic Regression & 0.8094 & 0.8063 & 0.8168 & 0.8292 & 0.7915 \\
            %         Random Forest & 0.8024 & 0.8146 & 0.8176 & 0.8255 & 0.7812 \\
            %         Gradient Boosting & 0.7766 & 0.7849 & 0.7878 & 0.8063 & 0.7530 \\
            %         SVM & 0.7631 & 0.7613 & 0.7802 & 0.7888 & 0.7428 \\
            %         KNN & 0.7485 & 0.7590 & 0.7656 & 0.7758 & 0.7221 \\
            %         Decision Tree & 0.7019 & 0.7069 & 0.7225 & 0.7175 & 0.6713 \\
            %         Naive Bayes & 0.6808 & 0.6833 & 0.7036 & 0.7245 & 0.6510 \\
            %         AdaBoost & 0.3869 & 0.3225 & 0.3970 & 0.3421 & 0.3465 \\
            %         \bottomrule
            %     \end{tabular}
            %     \caption{todo}
            % \end{table}

            %! Feature Engineering Approach - Hold-Out
            % \begin{table}[ht]
            %     \centering
            %     \begin{tabular}{lrrrrr}
            %         \toprule
            %         \textbf{Model} & \textbf{Accuracy} & \textbf{F1 Score} & \textbf{Recall} & \textbf{Precision} & \textbf{MCC} \\
            %         \midrule
            %           LDA (HO) & 0.8049 & 0.8094 & 0.8085 & 0.8140 & 0.7823 \\
            %           LDA (CV) & 0.8217 & 0.8125 & 0.8183 & 0.8298 & 0.8040 \\
            %           Logistic Regression (HO) & 0.8000 & 0.8111 & 0.8141 & 0.8157 & 0.7777 \\
            %           Logistic Regression (CV) & 0.8094 & 0.8063 & 0.8168 & 0.8292 & 0.7915 \\
            %           Random Forest (HO) & 0.7805 & 0.7947 & 0.7936 & 0.8033 & 0.7558 \\
            %           Random Forest (CV) & 0.8024 & 0.8146 & 0.8176 & 0.8255 & 0.7812 \\
            %         \bottomrule
            %         \end{tabular}
            %     \caption{CV = Cross-Validation, HO = Hold-Out}
            % \end{table}

            %! No mat walk vs no hoop walk - Correct Approach
            % \begin{table}[htbp]
            %     \centering
            %     \begin{minipage}{\linewidth}
            %         \centering
            %         \begin{tabular}{lrrrrr}
            %             \toprule
            %             \textbf{Model} & \textbf{Accuracy} & \textbf{F1 Score} & \textbf{Recall} & \textbf{Precision} & \textbf{MCC} \\
            %             \midrule
            %             LDA & \textbf{0.9310} & \textbf{0.9257} & \textbf{0.9256} & \textbf{0.9420} & \textbf{0.9232} \\
            %             MLP & 0.9093 & 0.9121 & 0.9131 & 0.9245 & 0.8989 \\
            %             Logistic Regression & 0.9040 & 0.9010 & 0.9016 & 0.9235 & 0.8937 \\
            %             \bottomrule
            %         \end{tabular}
            %         \caption{Mat-Walk movement data removed from the dataset.}
            %         \vspace{5pt} 
            %         \begin{tabular}{lrrrrr}
            %             \toprule
            %             \textbf{Model} & \textbf{Accuracy} & \textbf{F1 Score} & \textbf{Recall} & \textbf{Precision} & \textbf{MCC} \\
            %             \midrule
            %             LDA & \textbf{0.9421} & \textbf{0.9407} & \textbf{0.9403} & \textbf{0.9515} & \textbf{0.9353} \\
            %             MLP & 0.9206 & 0.9269 & 0.9278 & 0.9365 & 0.9115 \\
            %             Logistic Regression & 0.9119 & 0.9181 & 0.9191 & 0.9292 & 0.9019 \\
            %             \bottomrule
            %         \end{tabular}
            %         \caption{Hoop-Walk movement data removed from the dataset.}
            %     \end{minipage}
            % \end{table}
    
    \section{Discussion}
            %! 

            %! Show why by removing mat walk and hoop walk the accuracy increased 
            % Cite the table where it shows that the accuracy increased by removing mat walk and hoop walk

            % Show images how similiar the two movements, use plots.

            %! Show why feature engineering is important and how it changes the accuracy of the models and the training time
            
            % Table that shows the training times for the various approaches.

    \cleardoublepage